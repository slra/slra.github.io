
<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="slra-agcd">1</a>]
</td>
<td class="bibtexitem">
K.&nbsp;Usevich and I.&nbsp;Markovsky.
 Variable projection methods for approximate (greatest) common divisor
  computations.
 <em>Theoretical Computer Science</em>, 2016.
[&nbsp;<a href="j_bib.html#slra-agcd">bib</a>&nbsp;| 
<a href="http://arxiv.org/pdf/1304.6962v1">pdf</a>&nbsp;| 
<a href="http://arxiv.org/abs/1304.6962">http</a>&nbsp;]
<blockquote>
We consider the problem of finding for a given <em>N</em>-tuple of polynomials the closest <em>N</em>-tuple that has a common divisor of degree at least <em>d</em>. Extended weighted Euclidean semi-norm of coefficients is used as a measure of closeness. Two equivalent formulations of the problem are considered: (i) direct optimization over common divisors and cofactors, and (ii) Sylvester lowrank approximation. We use the duality between least-squares and least-norm problems to show that (i) and (ii) are closely related to mosaic Hankel low-rank approximation. This allows us to apply recent results on complexity and accuracy of computations for mosaic Hankel low-rank approximation. We develop optimization methods based on the variable projection principle. These methods have linear complexity in the degrees of the polynomials if either <em>d</em> is small or <em>d</em> is of the same order as the degrees of the polynomials. We provide a software implementation that is based on a software package for structured low-rank approximation.
</blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="jan">2</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky.
 On the most powerful unfalsified model for data with missing values.
 <em>Systems &amp; Control Letters</em>, 2016.
[&nbsp;<a href="j_bib.html#jan">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.sysconle.2015.12.012">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/jan.pdf">pdf</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/mpum-md.tar">software</a>&nbsp;]
<blockquote>
The notion of the most powerful unfalsified model plays a key role in system identification. Since its introduction in the mid 80's, many methods have been developed for its numerical computation. All currently existing methods, however, assume that the given data is a /complete/ trajectory of the system. Motivated by the practical issues of data corruption due to failing sensors, transmission lines, or storage devices, we study the problem of computing the most powerful unfalsified model from data with missing values. We do not make assumptions about the nature or pattern of the missing values apart from the basic one that they are a part of a trajectory of a linear time-invariant system. The identification problem with missing data is equivalent to a Hankel structured low-rank matrix completion problem. The method proposed constructs rank deficient complete submatrices of the incomplete Hankel matrix. Under specified conditions the kernels of the submatrices form a nonminimal kernel representation of the data generating system. The final step of the algorithm is reduction of the nonminimal kernel representation to a minimal one. Apart from its practical relevance in identification, missing data is a useful concept in systems and control. Classic problems, such as simulation, filtering, and tracking control can be viewed as missing data estimation problems for a given system. The corresponding identification problems with missing data are &ldquo;data-driven&rdquo; equivalents of the classical simulation, filtering, and tracking control problems.
</blockquote>
<p><blockquote>
Keywords: most powerful unfalsified model, exact system identification, subspace methods, missing data, low-rank matrix completion.
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="sensor-ieee">3</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky.
 Comparison of adaptive and model-free methods for dynamic
  measurement.
 <em>IEEE Signal Proc. Letters</em>, 22:1094-1097, 2015.
[&nbsp;<a href="j_bib.html#sensor-ieee">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/LSP.2014.2388369">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/sensor-ieee.pdf">pdf</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/sensor-ieee-code.tar">software</a>&nbsp;]
<blockquote>
Dynamic measurement aims to improve the speed and accuracy characteristics of measurement devices by signal processing. State-of-the-art dynamic measurement methods are model-based adaptive methods, , 1) they estimate model parameters in real-time and 2) based on the identified model perform model-based signal processing. The proposed model-free method belongs to the class of the subspace identification methods. It computes directly the quantity of interest without an explicit parameter estimation. This allows efficient computation as well as applicability to general high order multivariable processes.
</blockquote>
<p><blockquote>
Keywords: subspace methods, total least squares, adaptive filtering, model-free signal processing.
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="slra-consistency">4</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky and R.&nbsp;Pintelon.
 Identification of linear time-invariant systems from multiple
  experiments.
 <em>IEEE Trans. Signal Process.</em>, 63(13):3549-3554, 2015.
[&nbsp;<a href="j_bib.html#slra-consistency">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TSP.2015.2428218">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/papers/slra-consistency.pdf">pdf</a>&nbsp;]
<blockquote>
A standard assumption for consistent estimation in the errors-in-variables setting is persistency of excitation of the noise free input signal. We relax this assumption by considering data from multiple experiments. Consistency is obtained asymptotically as the number of experiments tends to infinity. The main theoretical and algorithmic difficulties are related to the growing number of to-be-estimated initial conditions. The method proposed in the paper is based on analytic elimination of the initial conditions and optimization over the remaining parameters. The resulting estimator is consistent, however, achieving asymptotically efficiency remains an open problem.
</blockquote>
<p><blockquote>
Keywords: maximum likelihood system identification, sum-of-damped exponentials modeling, consistency, structured low-rank approximation
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="als-kdu">5</a>]
</td>
<td class="bibtexitem">
K.&nbsp;Usevich and I.&nbsp;Markovsky.
 Adjusted least squares fitting of algebraic hypersurfaces.
 <em>Linear Algebra Appl.</em>, 2015.
[&nbsp;<a href="j_bib.html#als-kdu">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.laa.2015.07.023">DOI</a>&nbsp;| 
<a href="http://arxiv.org/abs/1412.2291">pdf</a>&nbsp;]
<blockquote>
We consider the problem of fitting a set of points in Euclidean space by an algebraic hypersurface. We assume that points on a true hypersurface, described by a polynomial equation, are corrupted by zero mean independent Gaussian noise, and we estimate the coefficients of the true polynomial equation. The adjusted least squares estimator accounts for the bias present in the ordinary least squares estimator. The adjusted least squares estimator is based on constructing a quasi-Hankel matrix, which is a bias-corrected matrix of moments. For the case of unknown noise variance, the estimator is defined as a solution of a polynomial eigenvalue problem. In this paper, we present new results on invariance properties of the adjusted least squares estimator and an improved algorithm for computing the estimator for an arbitrary set of monomials in the polynomial equation.
</blockquote>
<p><blockquote>
Keywords: hypersurface fitting; curve fitting; statistical estimation, errors-in-variables; Quasi-Hankel matrix; Hermite polynomials; affine invariance; subspace clustering
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="sensor-cep">6</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky.
 An application of system identification in metrology.
 <em>Control Engineering Practice</em>, 43:85-93, 2015.
[&nbsp;<a href="j_bib.html#sensor-cep">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.conengprac.2015.07.001">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/sensor-cep.pdf">pdf</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/sensor-cep-experiments.html">software</a>&nbsp;]
<blockquote>
Dynamic measurement refers to problems in metrology aiming at modification of characteristics of measurement devices by signal processing. Prototypical dynamic measurement problems, used in the paper as illustrative examples, are speeding up thermometers and weight scales. The paper presents a system theoretic formalization of the dynamic measurement problem as an input estimation problem for a system with step input. If the process dynamics is a priori known, the dynamic measurement problem is equivalent to a state estimation problem and can be solved efficiently in the linear time-invariant case by the Kalman filter. In the case of unknown process dynamics, the dynamic measurement problem can be solved by adaptive filtering methods. A topic of current research, called data-driven dynamic measurement, is development of methods that compute the measurement quantity without explicitly identifying the process dynamics.
</blockquote>
<p><blockquote>
Keywords: system identification, behavioral approach, Kalman filtering, metrology, reproducible research
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="jss-kdu">7</a>]
</td>
<td class="bibtexitem">
N.&nbsp;Golyandina, A.&nbsp;Korobeynikov, A.&nbsp;Shlemov, and K.&nbsp;Usevich.
 Multivariate and 2D extensions of singular spectrum analysis with
  the Rssa Package.
 <em>Journal of Statistical Software</em>, 67(2), 2015.
[&nbsp;<a href="j_bib.html#jss-kdu">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.18637/jss.v067.i02">DOI</a>&nbsp;]
<blockquote>
Implementation of multivariate and 2D extensions of singular spectrum analysis (SSA) by means of the R package Rssa is considered. The extensions include MSSA for simultaneous analysis and forecasting of several time series and 2D-SSA for analysis of digital images. A new extension of 2D-SSA analysis called shaped 2D-SSA is introduced for analysis of images of arbitrary shape, not necessary rectangular. It is shown that implementation of shaped 2D-SSA can serve as a basis for implementation of MSSA and other generalizations. Efficient implementation of operations with Hankel and Hankel-block-Hankel matrices through the fast Fourier transform is suggested. Examples with code fragments in R, which explain the methodology and demonstrate the proper use of Rssa, are presented.
</blockquote>
<p><blockquote>
Keywords: analysis, decomposition, forecasting, image processing, R package, singular spectrum analysis, time series
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="DreIshSch15">8</a>]
</td>
<td class="bibtexitem">
P.&nbsp;Dreesen, M.&nbsp;Ishteva, and J.&nbsp;Schoukens.
 Decoupling multivariate polynomials using first-order information and
  tensor decompositions.
 <em>SIAM J. Matrix Anal. Appl.</em>, 36(2):864-879, 2015.
[&nbsp;<a href="j_bib.html#DreIshSch15">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1137/140991546">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~mishteva/papers/decoupling_multivariate_polynomials.pdf">pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="slra-efficient">9</a>]
</td>
<td class="bibtexitem">
K.&nbsp;Usevich and I.&nbsp;Markovsky.
 Variable projection for affinely structured low-rank approximation in
  weighted 2-norms.
 <em>J. Comput. Appl. Math.</em>, 272:430-448, 2014.
[&nbsp;<a href="j_bib.html#slra-efficient">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.cam.2013.04.034">DOI</a>&nbsp;| 
<a href="http://arxiv.org/pdf/1211.3938v2">pdf</a>&nbsp;| 
<a href="http://slra.github.io/software-slra.html">software</a>&nbsp;| 
<a href="http://arxiv.org/abs/1211.3938">http</a>&nbsp;]
<blockquote>
The structured low-rank approximation problem for general affine structures, weighted 2-norms and fixed elements is considered. The variable projection principle is used to reduce the dimensionality of the optimization problem. Algorithms for evaluation of the cost function, the gradient and an approximation of the Hessian are developed. For <em>m</em>&#215;<em>n</em> mosaic Hankel matrices the algorithms have complexity <em>O</em>(<em>m</em><sup>2</sup><em>n</em>).
</blockquote>
<p><blockquote>
Keywords: Structured low-rank approximation; Variable projection; Mosaic Hankel matrices; Weighted 2-norm; Fixed elements; Computational complexity
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="slra-software">10</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky and K.&nbsp;Usevich.
 Software for weighted structured low-rank approximation.
 <em>J. Comput. Appl. Math.</em>, 256:278-292, 2014.
[&nbsp;<a href="j_bib.html#slra-software">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.cam.2013.07.048">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/slra.pdf">pdf</a>&nbsp;| 
<a href="http://slra.github.io/software-slra.html">software</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/recent-publications.html">.html</a>&nbsp;]
<blockquote>
A software package is presented that computes locally optimal solutions to low-rank approximation problems with the following features:
<p><ul>
<em>mosaic Hankel structure</em> constraint on the approximating matrix,
<em>weighted 2-norm</em> approximation criterion,
<em>fixed elements</em> in the approximating matrix,
<em>missing elements</em> in the data matrix, and
<em>linear constraints</em> on an approximating matrix's left kernel basis.
</ul>
It implements a variable projection type algorithm and allows the user to choose standard local optimization methods for the solution of the parameter optimization problem. For an <em>m</em>&#215;<em>n</em> data matrix, with <em>n</em>&gt;<em>m</em>, the computational complexity of the cost function and derivative evaluation is&nbsp;<em>O</em>(<em>m</em><sup>2</sup><em>n</em>). The package is suitable for applications with <em>n</em><em>m</em>. In statistical estimation and data modeling-the main application areas of the package-<em>n</em><em>m</em> corresponds to modeling of large amount of data by a low-complexity model. Performance results on benchmark system identification problems from the database DAISY and approximate common divisor problems are presented.
</blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="overview">11</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky.
 Recent progress on variable projection methods for structured
  low-rank approximation.
 <em>Signal Processing</em>, 96PB:406-419, 2014.
[&nbsp;<a href="j_bib.html#overview">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.sigpro.2013.09.021">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/overview.pdf">pdf</a>&nbsp;| 
<a href="http://slra.github.io/software-slra.html">software</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/recent-publications.html">.html</a>&nbsp;]
<blockquote>
Rank deficiency of a data matrix is equivalent to the existence of an exact linear model for the data. For the purpose of linear static modeling, the matrix is unstructured and the corresponding modeling problem is an approximation of the matrix by another matrix of a lower rank. In the context of linear time-invariant dynamic models, the appropriate data matrix is Hankel and the corresponding modeling problems becomes structured low-rank approximation. Low-rank approximation has applications in: system identification; signal processing, machine learning, and computer algebra, where different types of structure and constraints occur. This paper gives an overview of recent progress in efficient local optimization algorithms for solving weighted mosaic-Hankel structured low-rank approximation problems. In addition, the data matrix may have missing elements and elements may be specified as exact. The described algorithms are implemented in a publicly available software package. Their application to system identification, approximate common divisor, and data-driven simulation problems is described in this paper and is illustrated by reproducible simulation examples. As a data modeling paradigm the low-rank approximation setting is closely related to the the behavioral approach in systems and control, total least squares, errors-in-variables modeling, principal component analysis, and rank minimization.
</blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="UM12-aut">12</a>]
</td>
<td class="bibtexitem">
K.&nbsp;Usevich and I.&nbsp;Markovsky.
 Optimization on a Grassmann manifold with application to system
  identification.
 <em>Automatica</em>, 50:1656-1662, 2014.
[&nbsp;<a href="j_bib.html#UM12-aut">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.automatica.2014.04.010">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~kusevich/preprints/usevich_markovsky_aut2012.pdf">pdf</a>&nbsp;| 
<a href="http://slra.github.io/software-slra.html">software</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~kusevich/preprints.html">.html</a>&nbsp;]
<blockquote>
In this paper, we consider the problem of optimization of a cost function on a Grassmann manifold. This problem appears in system identification in the behavioral setting, which is a structured low-rank approximation problem. We develop a new method for local optimization on the Grassmann manifold with switching coordinate charts. This method reduces the optimization problem on the manifold to an optimization problem in a bounded domain of an Euclidean space. Our experiments show that this method is competitive with state- of-the-art retraction-based methods. Compared to retraction-based methods, the proposed method allows to incorporate easily an arbitrary optimization method for solving the optimization subproblem in the Euclidean space.
</blockquote>
<p><blockquote>
Keywords: system identification, over-parameterized models, Grassmann manifold, coordinate charts, structured low-rank approximation, optimization
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="pltv">13</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky, J.&nbsp;Goos, K.&nbsp;Usevich, and R.&nbsp;Pintelon.
 Realization and identification of autonomous linear periodically
  time-varying systems.
 <em>Automatica</em>, 50:1632-1640, 2014.
[&nbsp;<a href="j_bib.html#pltv">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.automatica.2014.04.003">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/pltv-rev3.pdf">pdf</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/software/pltv-code.tgz">software</a>&nbsp;]
<blockquote>
Subsampling of a linear periodically time-varying system results in a collection of linear time-invariant systems with common poles. This key fact, known as &ldquo;lifting&rdquo;, is used in a two step realization method. The first step is the realization of the time-invariant dynamics (the lifted system). Computationally, this step is a rank-revealing factorization of a block-Hankel matrix. The second step derives a state space representation of the periodic time-varying system. It is shown that no extra computations are required in the second step. The computational complexity of the overall method is therefore equal to the complexity for the realization of the lifted system. A modification of the realization method is proposed, which makes the complexity independent of the parameter variation period. Replacing the rank-revealing factorization in the realization algorithm by structured low-rank approximation yields a maximum likelihood identification method. Existing methods for structured low-rank approximation are used to identify efficiently linear periodically time-varying system. These methods can deal with missing data.

</blockquote>
<p><blockquote>
Keywords: linear periodically time-varying systems, lifting, realization, Kung's algorithm, Hankel low-rank approximation, maximum likelihood estimation.
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="rslra">14</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Ishteva, K.&nbsp;Usevich, and I.&nbsp;Markovsky.
 Factorization approach to structured low-rank approximation with
  applications.
 <em>SIAM J. Matrix Anal. Appl.</em>, 35(3):1180-1204, 2014.
[&nbsp;<a href="j_bib.html#rslra">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1137/130931655">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/rslra.pdf">pdf</a>&nbsp;| 
<a href="http://slra.github.io/software-slra.html">software</a>&nbsp;]
<blockquote>
We consider the problem of approximating an affinely structured matrix, for example a Hankel matrix, by a low-rank matrix with the same structure. This problem occurs in system identification, signal processing and computer algebra, among others. We impose the low-rank by modeling the approximation as a product of two factors with reduced dimension. The structure of the low-rank model is enforced by introducing a regularization term in the objective function. The proposed local optimization algorithm is able to solve the weighted structured low-rank approximation problem, as well as to deal with the cases of missing or fixed elements. In contrast to approaches based on kernel representations (in linear algebraic sense), the proposed algorithm is designed to address the case of small targeted rank. We compare it to existing approaches on numerical examples of system identification, approximate greatest common divisor problem, and symmetric tensor decomposition and demonstrate its consistently good performance.
</blockquote>
<p><blockquote>
Keywords: low-rank approximation, affine structure, regularization, system identification, approximate greatest common divisor
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="rgtls">15</a>]
</td>
<td class="bibtexitem">
S.&nbsp;Rhode, K.&nbsp;Usevich, I.&nbsp;Markovsky, and F.&nbsp;Gauterin.
 A recursive restricted total least-squares algorithm.
 <em>IEEE Trans. Signal Process.</em>, 62(21):5652-5662, 2014.
[&nbsp;<a href="j_bib.html#rgtls">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1109/TSP.2014.2350959">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/rgtls.pdf">pdf</a>&nbsp;| 
<a href="http://ieeexplore.ieee.org/xpl/abstractMultimedia.jsp?arnumber=6882213">software</a>&nbsp;]
<blockquote>
We show that the generalized total least squares problem with a singular noise covariance matrix is equivalent to the restricted total least squares problem and propose a recursive method for its numerical solution. The method is based on the generalized inverse iteration. The estimation error covariance matrix and the estimated augmented correction are also characterized and computed recursively. The algorithm is cheap to compute and is suitable for online implementation. Simulation results in least squares, data least squares, total least squares, and restricted total least squares noise scenarios show fast convergence of the parameter estimates to their optimal values obtained by corresponding batch algorithms.
</blockquote>
<p><blockquote>
Keywords: total least squares, generalized total least squares, restricted total least squares (RTLS), recursive estimation, subspace tracking, system identification
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="DeMarchi.Usevich14LAaIA-certain">16</a>]
</td>
<td class="bibtexitem">
S.&nbsp;De Marchi and K.&nbsp;Usevich.
 On certain multivariate Vandermonde determinants whose variables
  separate.
 <em>Linear Algebra and Its Applications</em>, 449:17-27, 2014.
[&nbsp;<a href="j_bib.html#DeMarchi.Usevich14LAaIA-certain">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.laa.2014.01.034">DOI</a>&nbsp;]
<blockquote>
We prove that for almost square tensor product grids and certain sets of bivariate polynomials the Vandermonde determinant can be factored into a product of univariate Vandermonde determinants. This result generalizes the conjecture [Lemma 1, L. Bos et al. (2009), Dolomites Research Notes on Approximation, 2:1-15]. As a special case, we apply the result to Padua and Padua-like points.
</blockquote>
<p><blockquote>
Keywords:  Multivariate Vandermonde determinant; Padua points; Tensor product grid; Polynomial matrices; Semiseparable matrices
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="KanIshPar13">17</a>]
</td>
<td class="bibtexitem">
R.&nbsp;Kannan, M.&nbsp;Ishteva, and H.&nbsp;Park.
 Bounded matrix factorization for recommender system.
 <em>Knowledge and Information Systems</em>, 39(3):491-511, 2014.
[&nbsp;<a href="j_bib.html#KanIshPar13">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s10115-013-0710-2">DOI</a>&nbsp;| 
<a href="http://link.springer.com/content/pdf/10.1007%2Fs10115-013-0710-2.pdf">pdf</a>&nbsp;| 
<a href="http://link.springer.com/article/10.1007/s10115-013-0710-2">http</a>&nbsp;]
<blockquote>
Matrix factorization has been widely utilized as a latent factor model for solving the recommender system problem using collaborative filtering. For a recommender system, all the ratings in the rating matrix are bounded within a pre-determined range. In this paper, we propose a new improved matrix factorization approach for such a rating matrix, called Bounded Matrix Factorization (BMF), which imposes a lower and an upper bound on every estimated missing element of the rating matrix. We present an efficient algorithm to solve BMF based on the block coordinate descent method. We show that our algorithm is scalable for large matrices with missing elements on multicore systems with low memory. We present substantial experimental results illustrating that the proposed method outperforms the state of the art algorithms for recommender system such as stochastic gradient descent, alternating least squares with regularization, SVD++ and Bias-SVD on real-world datasets such as Jester, Movielens, Book crossing, Online dating and Netflix.
</blockquote>
<p><blockquote>
Keywords: Low-rank approximation; Recommender systems; Bound constraints; Matrix factorization; Block coordinate descent method; Scalable algorithm; Block
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="slra-ext">18</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky and K.&nbsp;Usevich.
 Structured low-rank approximation with missing data.
 <em>SIAM J. Matrix Anal. Appl.</em>, pages 814-830, 2013.
[&nbsp;<a href="j_bib.html#slra-ext">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1137/120883050">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/slra-ext.pdf">pdf</a>&nbsp;| 
<a href="http://slra.github.io/software-slra.html">software</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/recent-publications.html">.html</a>&nbsp;]
<blockquote>
We consider low-rank approximation of affinely structured matrices with missing elements. The method proposed is based on reformulation of the problem as inner and outer optimization. The inner minimization is a singular linear least-norm problem and admits an analytic solution. The outer problem is a nonlinear least squares problem and is solved by local optimization methods: minimization subject to quadratic equality constraints and unconstrained minimization with regularized cost function. The method is generalized to weighted low-rank approximation with missing values and is illustrated on approximate low-rank matrix completion, system identification, and data-driven simulation problems. An extended version of the paper is a literate program, implementing the method and reproducing the presented results.
</blockquote>
<p><blockquote>
Keywords: low-rank approximation, missing data, variable projection, system identification, approximate matrix completion.
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="ident">19</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky.
 A software package for system identification in the behavioral
  setting.
 <em>Control Engineering Practice</em>, 21:1422-1436, 2013.
[&nbsp;<a href="j_bib.html#ident">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.conengprac.2013.06.010">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/ident.pdf">pdf</a>&nbsp;| 
<a href="http://slra.github.io/software-ident.html">software</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/recent-publications.html">.html</a>&nbsp;]
<blockquote>
An identification problem with no a priori separation of the variables into inputs and outputs and representation invariant approximation criterion is considered. The model class consists of linear time-invariant systems of bounded complexity and the approximation criterion is the minimum of a weighted 2-norm distance between the given time series and a time series that is consistent with the model. The problem is equivalent to and is solved as a mosaic-Hankel structured low-rank approximation problem. Software implementing the approach is developed and tested on benchmark problems. Additional nonstandard features of the software are specification of exact and missing variables and identification from multiple experiments.
</blockquote>
<p><blockquote>
Keywords: system identification; model reduction; behavioral approach; missing data; low-rank approximation; total least squares; reproducible research; DAISY.
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="IshAbsVDo13">20</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Ishteva, P.-A. Absil, and P.&nbsp;Van Dooren.
 Jacobi algorithm for the best low multilinear rank approximation of
  symmetric tensors.
 <em>SIAM J. Matrix Anal. Appl.</em>, 34(2):651-672, 2013.
[&nbsp;<a href="j_bib.html#IshAbsVDo13">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1137/11085743X">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~mishteva/symmetric_tensor_approximations.pdf">pdf</a>&nbsp;| 
<a href="http://epubs.siam.org/doi/abs/10.1137/11085743X">http</a>&nbsp;]
<blockquote>
The problem discussed in this paper is the symmetric best low multilinear rank approximation of third-order symmetric tensors. We propose an algorithm based on Jacobi rotations, for which symmetry is preserved at each iteration. Two numerical examples are provided indicating the need of such algorithms. An important part of the paper consists of proving that our algorithm converges to stationary points of the objective function. This can be considered an advantage of the proposed algorithm over existing symmetry-preserving algorithms in the literature.
</blockquote>
<p><blockquote>
Keywords: multilinear algebra, higher-order tensor, rank reduction, singular value decomposition, Jacobi rotation.
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="LMFR12">21</a>]
</td>
<td class="bibtexitem">
F.&nbsp;Le, I.&nbsp;Markovsky, C.&nbsp;Freeman, and E.&nbsp;Rogers.
 Recursive identification of Hammerstein systems with application to
  electrically stimulated muscle.
 <em>Control Engineering Practice</em>, 20(4):386-396, 2012.
[&nbsp;<a href="j_bib.html#LMFR12">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.conengprac.2011.08.001">DOI</a>&nbsp;| 
<a href="http://eprints.soton.ac.uk/271583/3/zoe2-published.pdf">pdf</a>&nbsp;]
<blockquote>
Two methods for recursive identification of Hammerstein systems are presented. In the first method, recursive least squares algorithm is applied to an overparameterized representation of the Hammerstein model and a rank-1 approximation is used to recover the linear and nonlinear parameters from the estimated overparameterized representation. In the second method, the linear and nonlinear parameters are recursively estimated in an alternate manner. Numerical example with simulated data and experimental data from human muscles show the superiority of second method.
</blockquote>
<p><blockquote>
Keywords: recursive identification; Hammerstein system; muscle model; functional electrical stimulation.
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="complex-ls">22</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky.
 On the complex least squares problem with constrained phase.
 <em>SIAM J. Matrix Anal. Appl.</em>, 32(3):987-992, 2011.
[&nbsp;<a href="j_bib.html#complex-ls">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1137/110826497">DOI</a>&nbsp;| 
<a href="http://eprints.soton.ac.uk/272534/1/complex-ls.pdf">pdf</a>&nbsp;| 
<a href="http://eprints.soton.ac.uk/272534/5/complex-ls-code.tar">software</a>&nbsp;]
<blockquote>
The problem of solving approximately in the least squares sense an overdetermined linear system of equations with complex valued coefficients is considered, where the elements of the solution vector are constrained to have the same phase. A direct solution to this problem is given in [Linear Algebra and Its Applications, Vol. 433, pp.&nbsp;1719-1721]. An alternative direct solution that reduces the problem to a generalized eigenvalue problem is derived in this paper. The new solution is related to generalized low-rank matrix approximation and makes possible one to use existing robust and efficient algorithms.
</blockquote>
<p><blockquote>
Keywords: Linear system of equations, Phase constraint, Low-rank approximation, Total least squares.
</blockquote>

</td>
</tr>
</table><hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.97.</em></p>
