
<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="slra-efficient">1</a>]
</td>
<td class="bibtexitem">
K.&nbsp;Usevich and I.&nbsp;Markovsky.
 Variable projection for affinely structured low-rank approximation in
  weighted 2-norms.
 <em>J. Comput. Appl. Math.</em>, 272:430-448, 2014.
[&nbsp;<a href="j_bib.html#slra-efficient">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.cam.2013.04.034">DOI</a>&nbsp;| 
<a href="http://arxiv.org/pdf/1211.3938v2">pdf</a>&nbsp;| 
<a href="http://arxiv.org/abs/1211.3938">http</a>&nbsp;]
<blockquote>
The structured low-rank approximation problem for general affine structures, weighted 2-norms and fixed elements is considered. The variable projection principle is used to reduce the dimensionality of the optimization problem. Algorithms for evaluation of the cost function, the gradient and an approximation of the Hessian are developed. For <em>m</em>&#215;<em>n</em> mosaic Hankel matrices the algorithms have complexity <em>O</em>(<em>m</em><sup>2</sup><em>n</em>).
</blockquote>
<p><blockquote>
Keywords: Structured low-rank approximation; Variable projection; Mosaic Hankel matrices; Weighted 2-norm; Fixed elements; Computational complexity
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="slra-software">2</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky and K.&nbsp;Usevich.
 Software for weighted structured low-rank approximation.
 <em>J. Comput. Appl. Math.</em>, 256:278-292, 2014.
[&nbsp;<a href="j_bib.html#slra-software">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.cam.2013.07.048">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/slra.pdf">pdf</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/recent-publications.html">.html</a>&nbsp;]
<blockquote>
A software package is presented that computes locally optimal solutions to low-rank approximation problems with the following features:
<p><ul>
<em>mosaic Hankel structure</em> constraint on the approximating matrix,
<em>weighted 2-norm</em> approximation criterion,
<em>fixed elements</em> in the approximating matrix,
<em>missing elements</em> in the data matrix, and
<em>linear constraints</em> on an approximating matrix's left kernel basis.
</ul>
It implements a variable projection type algorithm and allows the user to choose standard local optimization methods for the solution of the parameter optimization problem. For an <em>m</em>&#215;<em>n</em> data matrix, with <em>n</em>&gt;<em>m</em>, the computational complexity of the cost function and derivative evaluation is&nbsp;<em>O</em>(<em>m</em><sup>2</sup><em>n</em>). The package is suitable for applications with <em>n</em><em>m</em>. In statistical estimation and data modeling-the main application areas of the package-<em>n</em><em>m</em> corresponds to modeling of large amount of data by a low-complexity model. Performance results on benchmark system identification problems from the database DAISY and approximate common divisor problems are presented.
</blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="overview">3</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky.
 Recent progress on variable projection methods for structured
  low-rank approximation.
 <em>Signal Processing</em>, 96PB:406-419, 2014.
[&nbsp;<a href="j_bib.html#overview">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.sigpro.2013.09.021">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/overview.pdf">pdf</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/recent-publications.html">.html</a>&nbsp;]
<blockquote>
Rank deficiency of a data matrix is equivalent to the existence of an exact linear model for the data. For the purpose of linear static modeling, the matrix is unstructured and the corresponding modeling problem is an approximation of the matrix by another matrix of a lower rank. In the context of linear time-invariant dynamic models, the appropriate data matrix is Hankel and the corresponding modeling problems becomes structured low-rank approximation. Low-rank approximation has applications in: system identification; signal processing, machine learning, and computer algebra, where different types of structure and constraints occur. This paper gives an overview of recent progress in efficient local optimization algorithms for solving weighted mosaic-Hankel structured low-rank approximation problems. In addition, the data matrix may have missing elements and elements may be specified as exact. The described algorithms are implemented in a publicly available software package. Their application to system identification, approximate common divisor, and data-driven simulation problems is described in this paper and is illustrated by reproducible simulation examples. As a data modeling paradigm the low-rank approximation setting is closely related to the the behavioral approach in systems and control, total least squares, errors-in-variables modeling, principal component analysis, and rank minimization.
</blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="UM12-aut">4</a>]
</td>
<td class="bibtexitem">
K.&nbsp;Usevich and I.&nbsp;Markovsky.
 Optimization on a Grassmann manifold with application to system
  identification.
 <em>Automatica</em>, 50:1656â€“-1662, 2014.
[&nbsp;<a href="j_bib.html#UM12-aut">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.automatica.2014.04.010">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~kusevich/preprints/usevich_markovsky_aut2012.pdf">pdf</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~kusevich/preprints.html">.html</a>&nbsp;]
<blockquote>
In this paper, we consider the problem of optimization of a cost function on a Grassmann manifold. This problem appears in system identification in the behavioral setting, which is a structured low-rank approximation problem. We develop a new method for local optimization on the Grassmann manifold with switching coordinate charts. This method reduces the optimization problem on the manifold to an optimization problem in a bounded domain of an Euclidean space. Our experiments show that this method is competitive with state- of-the-art retraction-based methods. Compared to retraction-based methods, the proposed method allows to incorporate easily an arbitrary optimization method for solving the optimization subproblem in the Euclidean space.
</blockquote>
<p><blockquote>
Keywords: system identification, over-parameterized models, Grassmann manifold, coordinate charts, structured low-rank approximation, optimization
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="pltv">5</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky, J.&nbsp;Goos, K.&nbsp;Usevich, and R.&nbsp;Pintelon.
 Realization and identification of autonomous linear periodically
  time-varying systems.
 <em>Automatica</em>, 50:1632-1640, 2014.
[&nbsp;<a href="j_bib.html#pltv">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.automatica.2014.04.003">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/pltv-rev3.pdf">pdf</a>&nbsp;]
<blockquote>
Subsampling of a linear periodically time-varying system results in a collection of linear time-invariant systems with common poles. This key fact, known as &ldquo;lifting&rdquo;, is used in a two step realization method. The first step is the realization of the time-invariant dynamics (the lifted system). Computationally, this step is a rank-revealing factorization of a block-Hankel matrix. The second step derives a state space representation of the periodic time-varying system. It is shown that no extra computations are required in the second step. The computational complexity of the overall method is therefore equal to the complexity for the realization of the lifted system. A modification of the realization method is proposed, which makes the complexity independent of the parameter variation period. Replacing the rank-revealing factorization in the realization algorithm by structured low-rank approximation yields a maximum likelihood identification method. Existing methods for structured low-rank approximation are used to identify efficiently linear periodically time-varying system. These methods can deal with missing data.

</blockquote>
<p><blockquote>
Keywords: linear periodically time-varying systems, lifting, realization, Kung's algorithm, Hankel low-rank approximation, maximum likelihood estimation.
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="rslra">6</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Ishteva, K.&nbsp;Usevich, and I.&nbsp;Markovsky.
 Factorization approach to structured low-rank approximation with
  applications.
 <em>SIAM J. Matrix Anal. Appl.</em>, 35(3):1180-1204, 2014.
[&nbsp;<a href="j_bib.html#rslra">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1137/130931655">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/rslra.pdf">pdf</a>&nbsp;]
<blockquote>
We consider the problem of approximating an affinely structured matrix, for example a Hankel matrix, by a low-rank matrix with the same structure. This problem occurs in system identification, signal processing and computer algebra, among others. We impose the low-rank by modeling the approximation as a product of two factors with reduced dimension. The structure of the low-rank model is enforced by introducing a regularization term in the objective function. The proposed local optimization algorithm is able to solve the weighted structured low-rank approximation problem, as well as to deal with the cases of missing or fixed elements. In contrast to approaches based on kernel representations (in linear algebraic sense), the proposed algorithm is designed to address the case of small targeted rank. We compare it to existing approaches on numerical examples of system identification, approximate greatest common divisor problem, and symmetric tensor decomposition and demonstrate its consistently good performance.
</blockquote>
<p><blockquote>
Keywords: low-rank approximation, affine structure, regularization, system identification, approximate greatest common divisor
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="rgtls">7</a>]
</td>
<td class="bibtexitem">
S.&nbsp;Rhode, K.&nbsp;Usevich, I.&nbsp;Markovsky, and F.&nbsp;Gauterin.
 A recursive restricted total least-squares algorithm.
 <em>IEEE Trans. Signal Process.</em>, 2014.
[&nbsp;<a href="j_bib.html#rgtls">bib</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/rgtls.pdf">pdf</a>&nbsp;]
<blockquote>
We show that the generalized total least squares (GTLS) problem with a singular noise covariance matrix is equivalent to the restricted total least squares (RTLS) problem and propose a recursive method for its numerical solution. The method is based on the generalized inverse iteration. The estimation error covariance matrix and the estimated augmented correction are also characterized and computed recursively. The algorithm is cheap to compute and is suitable for online implementation. Simulation results in least squares (LS), data least squares (DLS), total least squares (TLS), and RTLS noise scenarios show fast convergence of the parameter estimates to their optimal values obtained by corresponding batch algorithms.
</blockquote>
<p><blockquote>
Keywords: total least squares (TLS), generalized total least squares (GTLS), restricted total least squares (RTLS), recursive estimation, subspace tracking, system identification
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="DeMarchi.Usevich14LAaIA-certain">8</a>]
</td>
<td class="bibtexitem">
S.&nbsp;De Marchi and K.&nbsp;Usevich.
 On certain multivariate Vandermonde determinants whose variables
  separate.
 <em>Linear Algebra and Its Applications</em>, 449:17-27, 2014.
[&nbsp;<a href="j_bib.html#DeMarchi.Usevich14LAaIA-certain">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.laa.2014.01.034">DOI</a>&nbsp;]
<blockquote>
We prove that for almost square tensor product grids and certain sets of bivariate polynomials the Vandermonde determinant can be factored into a product of univariate Vandermonde determinants. This result generalizes the conjecture [Lemma 1, L. Bos et al. (2009), Dolomites Research Notes on Approximation, 2:1-15]. As a special case, we apply the result to Padua and Padua-like points.
</blockquote>
<p><blockquote>
Keywords:  Multivariate Vandermonde determinant; Padua points; Tensor product grid; Polynomial matrices; Semiseparable matrices
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="KanIshPar13">9</a>]
</td>
<td class="bibtexitem">
R.&nbsp;Kannan, M.&nbsp;Ishteva, and H.&nbsp;Park.
 Bounded matrix factorization for recommender system.
 <em>Knowledge and Information Systems</em>, 39(3):491-511, 2014.
[&nbsp;<a href="j_bib.html#KanIshPar13">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1007/s10115-013-0710-2">DOI</a>&nbsp;| 
<a href="http://link.springer.com/content/pdf/10.1007%2Fs10115-013-0710-2.pdf">pdf</a>&nbsp;| 
<a href="http://link.springer.com/article/10.1007/s10115-013-0710-2">http</a>&nbsp;]
<blockquote>
Matrix factorization has been widely utilized as a latent factor model for solving the recommender system problem using collaborative filtering. For a recommender system, all the ratings in the rating matrix are bounded within a pre-determined range. In this paper, we propose a new improved matrix factorization approach for such a rating matrix, called Bounded Matrix Factorization (BMF), which imposes a lower and an upper bound on every estimated missing element of the rating matrix. We present an efficient algorithm to solve BMF based on the block coordinate descent method. We show that our algorithm is scalable for large matrices with missing elements on multicore systems with low memory. We present substantial experimental results illustrating that the proposed method outperforms the state of the art algorithms for recommender system such as stochastic gradient descent, alternating least squares with regularization, SVD++ and Bias-SVD on real-world datasets such as Jester, Movielens, Book crossing, Online dating and Netflix.
</blockquote>
<p><blockquote>
Keywords: Low-rank approximation; Recommender systems; Bound constraints; Matrix factorization; Block coordinate descent method; Scalable algorithm; Block
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="slra-ext">10</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky and K.&nbsp;Usevich.
 Structured low-rank approximation with missing data.
 <em>SIAM J. Matrix Anal. Appl.</em>, pages 814-830, 2013.
[&nbsp;<a href="j_bib.html#slra-ext">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1137/120883050">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/slra-ext.pdf">pdf</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/recent-publications.html">.html</a>&nbsp;]
<blockquote>
We consider low-rank approximation of affinely structured matrices with missing elements. The method proposed is based on reformulation of the problem as inner and outer optimization. The inner minimization is a singular linear least-norm problem and admits an analytic solution. The outer problem is a nonlinear least squares problem and is solved by local optimization methods: minimization subject to quadratic equality constraints and unconstrained minimization with regularized cost function. The method is generalized to weighted low-rank approximation with missing values and is illustrated on approximate low-rank matrix completion, system identification, and data-driven simulation problems. An extended version of the paper is a literate program, implementing the method and reproducing the presented results.
</blockquote>
<p><blockquote>
Keywords: low-rank approximation, missing data, variable projection, system identification, approximate matrix completion.
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="ident">11</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky.
 A software package for system identification in the behavioral
  setting.
 <em>Control Engineering Practice</em>, 21:1422-1436, 2013.
[&nbsp;<a href="j_bib.html#ident">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.conengprac.2013.06.010">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/publications/ident.pdf">pdf</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~imarkovs/recent-publications.html">.html</a>&nbsp;]
<blockquote>
An identification problem with no a priori separation of the variables into inputs and outputs and representation invariant approximation criterion is considered. The model class consists of linear time-invariant systems of bounded complexity and the approximation criterion is the minimum of a weighted 2-norm distance between the given time series and a time series that is consistent with the model. The problem is equivalent to and is solved as a mosaic-Hankel structured low-rank approximation problem. Software implementing the approach is developed and tested on benchmark problems. Additional nonstandard features of the software are specification of exact and missing variables and identification from multiple experiments.
</blockquote>
<p><blockquote>
Keywords: system identification; model reduction; behavioral approach; missing data; low-rank approximation; total least squares; reproducible research; DAISY.
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="IshAbsVDo13">12</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Ishteva, P.-A. Absil, and P.&nbsp;Van Dooren.
 Jacobi algorithm for the best low multilinear rank approximation of
  symmetric tensors.
 <em>SIAM J. Matrix Anal. Appl.</em>, 34(2):651-672, 2013.
[&nbsp;<a href="j_bib.html#IshAbsVDo13">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1137/11085743X">DOI</a>&nbsp;| 
<a href="http://homepages.vub.ac.be/~mishteva/symmetric_tensor_approximations.pdf">pdf</a>&nbsp;| 
<a href="http://epubs.siam.org/doi/abs/10.1137/11085743X">http</a>&nbsp;]
<blockquote>
The problem discussed in this paper is the symmetric best low multilinear rank approximation of third-order symmetric tensors. We propose an algorithm based on Jacobi rotations, for which symmetry is preserved at each iteration. Two numerical examples are provided indicating the need of such algorithms. An important part of the paper consists of proving that our algorithm converges to stationary points of the objective function. This can be considered an advantage of the proposed algorithm over existing symmetry-preserving algorithms in the literature.
</blockquote>
<p><blockquote>
Keywords: multilinear algebra, higher-order tensor, rank reduction, singular value decomposition, Jacobi rotation.
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="LMFR12">13</a>]
</td>
<td class="bibtexitem">
F.&nbsp;Le, I.&nbsp;Markovsky, C.&nbsp;Freeman, and E.&nbsp;Rogers.
 Recursive identification of Hammerstein systems with application to
  electrically stimulated muscle.
 <em>Control Engineering Practice</em>, 20(4):386-396, 2012.
[&nbsp;<a href="j_bib.html#LMFR12">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1016/j.conengprac.2011.08.001">DOI</a>&nbsp;| 
<a href="http://eprints.soton.ac.uk/271583/3/zoe2-published.pdf">pdf</a>&nbsp;]
<blockquote>
Two methods for recursive identification of Hammerstein systems are presented. In the first method, recursive least squares algorithm is applied to an overparameterized representation of the Hammerstein model and a rank-1 approximation is used to recover the linear and nonlinear parameters from the estimated overparameterized representation. In the second method, the linear and nonlinear parameters are recursively estimated in an alternate manner. Numerical example with simulated data and experimental data from human muscles show the superiority of second method.
</blockquote>
<p><blockquote>
Keywords: recursive identification; Hammerstein system; muscle model; functional electrical stimulation.
</blockquote>

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="complex-ls">14</a>]
</td>
<td class="bibtexitem">
I.&nbsp;Markovsky.
 On the complex least squares problem with constrained phase.
 <em>SIAM J. Matrix Anal. Appl.</em>, 32(3):987-992, 2011.
[&nbsp;<a href="j_bib.html#complex-ls">bib</a>&nbsp;| 
<a href="http://dx.doi.org/10.1137/110826497">DOI</a>&nbsp;| 
<a href="http://eprints.soton.ac.uk/272534/1/complex-ls.pdf">pdf</a>&nbsp;| 
<a href="http://eprints.soton.ac.uk/272534/5/complex-ls-code.tar">software</a>&nbsp;]
<blockquote>
The problem of solving approximately in the least squares sense an overdetermined linear system of equations with complex valued coefficients is considered, where the elements of the solution vector are constrained to have the same phase. A direct solution to this problem is given in [Linear Algebra and Its Applications, Vol. 433, pp.&nbsp;1719-1721]. An alternative direct solution that reduces the problem to a generalized eigenvalue problem is derived in this paper. The new solution is related to generalized low-rank matrix approximation and makes possible one to use existing robust and efficient algorithms.
</blockquote>
<p><blockquote>
Keywords: Linear system of equations, Phase constraint, Low-rank approximation, Total least squares.
</blockquote>

</td>
</tr>
</table><hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.97.</em></p>
